mamba takes parameters called expand and d_state.

what mamba does is take in the input as (b, seq_len, d_model). it then expands the input as: d_inner = d_model*expand. then each of this single expanded representation
is taken as an independent channel or say that d_inner of parallel channels are made. each channel gets it's own ssm parameters (A, B, C) and hidden state of size
d_state.

hence, there will be d_inner number of hidden states of size d_state.

expanding and using parallel channel somehow increases expressiveness.

mamba uses depth wise convolutions:

self.conv1d = nn.Conv1d(
    in_channels=self.d_inner,
    out_channels=self.d_inner,
    groups=self.d_inner  # <- depthwise
)

Depthwise means:

each of the d_inner feature streams is processed independently

hence each is treated like a channel.