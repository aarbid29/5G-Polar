{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "589f41f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anjal/Desktop/Code/mamba-seq-pred/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dataset import PolarDecDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from models.wrappers.mamba_32bits import MambaPolarDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d85178a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 32\n",
    "CONFIG_NO = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e6fb613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78f1e95",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36e1bd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PolarDecDataset(snr_db=10, num_samples=100000)\n",
    "test_set = PolarDecDataset(snr_db=10, num_samples=3200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdad0b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset, batch_size = 32)\n",
    "test_dataloader = DataLoader(test_set, batch_size = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbbd230",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17a2a925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MambaPolarDecoder(\n",
       "  (discrete_embedding): Embedding(2, 32)\n",
       "  (linear_embedding1): Linear(in_features=1, out_features=32, bias=True)\n",
       "  (linear_embedding2): Linear(in_features=1, out_features=32, bias=True)\n",
       "  (linear_input_layer): Linear(in_features=96, out_features=32, bias=True)\n",
       "  (encoder_layers): ModuleList(\n",
       "    (0-1): 2 x BiMambaEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-7): 8 x BiMambaBlock(\n",
       "          (pre_ln_f): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (mamba_f): Mamba(\n",
       "            (in_proj): Linear(in_features=32, out_features=128, bias=False)\n",
       "            (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)\n",
       "            (act): SiLU()\n",
       "            (x_proj): Linear(in_features=64, out_features=34, bias=False)\n",
       "            (dt_proj): Linear(in_features=2, out_features=64, bias=True)\n",
       "            (out_proj): Linear(in_features=64, out_features=32, bias=False)\n",
       "          )\n",
       "          (post_ln_f): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (ffn_f): Sequential(\n",
       "            (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=128, out_features=32, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (pre_ln_r): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_ln_r): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (mamba_r): Mamba(\n",
       "            (in_proj): Linear(in_features=32, out_features=128, bias=False)\n",
       "            (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)\n",
       "            (act): SiLU()\n",
       "            (x_proj): Linear(in_features=64, out_features=34, bias=False)\n",
       "            (dt_proj): Linear(in_features=2, out_features=64, bias=True)\n",
       "            (out_proj): Linear(in_features=64, out_features=32, bias=False)\n",
       "          )\n",
       "          (ffn_r): Sequential(\n",
       "            (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=128, out_features=32, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "  (post_norms): ModuleList(\n",
       "    (0-1): 2 x LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (final_proj_layer): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MambaPolarDecoder(\n",
    "    d_model=32,\n",
    "    num_layer_encoder=2,\n",
    "    num_layers_bimamba_block=8,\n",
    "    seq_len=N,\n",
    "    d_state=16,\n",
    "    d_conv=4,\n",
    "    expand=2\n",
    ").to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81b5ae3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run/Change if loading an old model only\n",
    "\n",
    "model_path = \"./checkpoints/config_3/model_epoch_29.pt\"\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "\n",
    "state_dict = checkpoint.get(\"model_state_dict\", checkpoint.get(\"state_dict\", checkpoint))\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f20a282",
   "metadata": {},
   "source": [
    "## Minor modification to the Loss Function: Calculates loss only at non frozen positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05e72cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(frozen_bit_prior, target_vector, predicted_vector,  reliable_only=False):\n",
    "    \"\"\"\n",
    "    frozen_bit_prior: tensor of shape (seq_len,) with 1 for frozen, 0 for message bits\n",
    "    target_vector: tensor of shape (seq_len,)\n",
    "    predicted_vector: tensor of shape (seq_len,)\n",
    "    loss_fn: PyTorch loss function\n",
    "    \"\"\"\n",
    "\n",
    "    if reliable_only: \n",
    "     mask = (frozen_bit_prior != 1) \n",
    "     target_vector = target_vector[mask]\n",
    "     predicted_vector = predicted_vector[mask]\n",
    "\n",
    "    #print(\"target vector:\" ,target_vector[:32], \"\\n\")\n",
    "    #print(\"pred vector:\" ,predicted_vector[:32])\n",
    "\n",
    "   # print(f\"Length of reliable bits: {len(reliable_target)}\")\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    return loss_fn(predicted_vector, target_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808bd9a5",
   "metadata": {},
   "source": [
    "## Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5dc9380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted bits:01111101101111100111000100100000\n",
      "\n",
      "Actual bits: 11010101011011100011000010100000\n",
      "\n",
      "Loss: 0.4689653217792511\n"
     ]
    }
   ],
   "source": [
    "llr, frozen_tensor, snr_tensor, target_tensor= next(iter(train_dataloader))\n",
    "ip1 = llr.float().to(device)\n",
    "ip2 = frozen_tensor.int().to(device)\n",
    "ip3 = snr_tensor.float().to(device)\n",
    "\n",
    "predicted = model(ip1, ip2, ip3) #works\n",
    "\n",
    "loss = calculate_loss(ip2, target_tensor.to(device), predicted.to(device)) #works\n",
    "\n",
    "#print(f\"Channel Observation Vector: {channel_tensor}\\n\\n\")\n",
    "\n",
    "\n",
    "#print(f\"Channel Observation Vector: {ip1.shape}\\nFrozen Tensor: {ip2.shape}\\n\")\n",
    "#print(f\"Predicted Channel Input Vector(logits): {predicted.shape}\\n\\n\")\n",
    "\n",
    "#print(f\"Predicted (sigmoid): {torch.sigmoid(predicted)}\\n\\n\")\n",
    "pred = (torch.sigmoid(predicted) > 0.5).long()[0]\n",
    "\n",
    "print(f\"Predicted bits:{''.join(map(str, pred.cpu().tolist()))}\\n\")\n",
    "print(f\"Actual bits: {''.join(str(int(i)) for i in target_tensor[0])}\\n\")\n",
    "print(f\"Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "69534924",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=0.001)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', \n",
    "                                                        factor=0.5, patience=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32f8dbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index):\n",
    "\n",
    "    running_loss = 0\n",
    "    last_loss = 0\n",
    "\n",
    "\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        channel_tensor, frozen_tensor, snr_tensor, target_tensor = data\n",
    "        ip1 = channel_tensor.float().to(device)\n",
    "        ip2 = frozen_tensor.int().to(device)\n",
    "        ip3 = snr_tensor.float().to(device)\n",
    "        op = target_tensor.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(ip1,ip2 ,ip3 ).to(device)\n",
    "\n",
    "    #    B, L, C = outputs.shape\n",
    "    #    output_logits = outputs.view(B*L, C).to(device)\n",
    "    #    target_flattened = shifted.view(B*L).to(device).long()\n",
    "\n",
    "\n",
    "    #    loss = loss_fn(output_logits, target_flattened)\n",
    "        \n",
    "        loss = calculate_loss(ip2, op, outputs)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i%1000 == 999:\n",
    "            last_loss = running_loss/1000\n",
    "            print('  batch {} loss: {}\\n'.format(i + 1, last_loss))\n",
    "          #  print(f\"Predictions look currently like: {outputs[:32]}\\n\\n\")\n",
    "            running_loss = 0.\n",
    "    return last_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f26fb8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train( epochs=50):\n",
    "\n",
    "    best_vloss = 1_000_000.\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "     print('EPOCH {}:'.format(epoch + 1))\n",
    " \n",
    "   \n",
    "     model.train(True)\n",
    "     avg_loss = train_one_epoch(epoch)\n",
    "\n",
    "\n",
    "     running_vloss = 0.0\n",
    "    \n",
    "     model.eval()\n",
    "\n",
    "   \n",
    "     with torch.no_grad():\n",
    "        for i, vdata in enumerate(test_dataloader):\n",
    "            vchannel_tensor, vfrozen_tensor, vsnr_tensor, vtarget_tensor = vdata\n",
    "            voutputs = model(vchannel_tensor.float().to(device), vfrozen_tensor.int().to(device), vsnr_tensor.float().to(device))\n",
    "          #  B, L, C = voutputs.shape\n",
    "          #  vloss = loss_fn(voutputs.view(B*L, C).to(device), vlabels.view(B*L).to(device))\n",
    "            \n",
    "            vloss = calculate_loss(vfrozen_tensor.to(device), vtarget_tensor.to(device), voutputs.to(device))\n",
    "            running_vloss += vloss\n",
    "\n",
    "     avg_vloss = running_vloss / (i + 1)\n",
    "     print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    \n",
    "     scheduler.step(avg_vloss)\n",
    "\n",
    "    \n",
    "     if avg_vloss < best_vloss:\n",
    "      \n",
    "    \n",
    "      best_vloss = avg_vloss\n",
    "      model_path = f'./checkpoints/config_{CONFIG_NO}/model_epoch_{epoch}.pt'\n",
    "      torch.save({\n",
    "         \"comments\": \"New lightweight configuration used. Used 2 encoder layers instead of 1. Now each encoder layer has 8 bimamba blocks\",\n",
    "    'model_config': {\n",
    "        \"d_model\": model.d_model,\n",
    "        \"num_layer_encoder\": model.num_layer_encoder,\n",
    "        \"num_layers_bimamba_block\": model.num_layers_bimamba_block,\n",
    "        \"seq_len\": model.seq_len,\n",
    "        \"d_state\": model.d_state,\n",
    "        \"d_conv\": model.d_conv,\n",
    "        \"expand\": model.expand,\n",
    "    },\n",
    "    'epoch': epoch + 1,\n",
    "    'train_loss': avg_loss,\n",
    "    'val_loss': avg_vloss,\n",
    "    'state_dict': model.state_dict()\n",
    "}, model_path)\n",
    "\n",
    "\n",
    "     \n",
    "    print(\"Training completed. Model available to use\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6d589f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 1000 loss: 0.21762221300601958\n",
      "\n",
      "  batch 2000 loss: 0.18520276387035847\n",
      "\n",
      "  batch 3000 loss: 0.17971488001942634\n",
      "\n",
      "LOSS train 0.17971488001942634 valid 0.17469437420368195\n",
      "EPOCH 2:\n",
      "  batch 1000 loss: 0.17638983738422392\n",
      "\n",
      "  batch 2000 loss: 0.17330959343910218\n",
      "\n",
      "  batch 3000 loss: 0.17274228969216346\n",
      "\n",
      "LOSS train 0.17274228969216346 valid 0.16979952156543732\n",
      "EPOCH 3:\n",
      "  batch 1000 loss: 0.17128088223934174\n",
      "\n",
      "  batch 2000 loss: 0.16987204115092755\n",
      "\n",
      "  batch 3000 loss: 0.16895946999639272\n",
      "\n",
      "LOSS train 0.16895946999639272 valid 0.16394193470478058\n",
      "EPOCH 4:\n",
      "  batch 1000 loss: 0.16740853187441826\n",
      "\n",
      "  batch 2000 loss: 0.16658798415213824\n",
      "\n",
      "  batch 3000 loss: 0.164609590344131\n",
      "\n",
      "LOSS train 0.164609590344131 valid 0.165303573012352\n",
      "EPOCH 5:\n",
      "  batch 1000 loss: 0.16448793176561594\n",
      "\n",
      "  batch 2000 loss: 0.1641144694760442\n",
      "\n",
      "  batch 3000 loss: 0.16233833894133567\n",
      "\n",
      "LOSS train 0.16233833894133567 valid 0.15812137722969055\n",
      "EPOCH 6:\n",
      "  batch 1000 loss: 0.1625214083045721\n",
      "\n",
      "  batch 2000 loss: 0.16258501040935516\n",
      "\n",
      "  batch 3000 loss: 0.16065794184803964\n",
      "\n",
      "LOSS train 0.16065794184803964 valid 0.1597515195608139\n",
      "EPOCH 7:\n",
      "  batch 1000 loss: 0.15902140217274427\n",
      "\n",
      "  batch 2000 loss: 0.15967778182029724\n",
      "\n",
      "  batch 3000 loss: 0.1566896220445633\n",
      "\n",
      "LOSS train 0.1566896220445633 valid 0.1527092158794403\n",
      "EPOCH 8:\n",
      "  batch 1000 loss: 0.1533303684592247\n",
      "\n",
      "  batch 2000 loss: 0.15213744775950908\n",
      "\n",
      "  batch 3000 loss: 0.15132653461396695\n",
      "\n",
      "LOSS train 0.15132653461396695 valid 0.14878569543361664\n",
      "EPOCH 9:\n",
      "  batch 1000 loss: 0.15155035646259785\n",
      "\n",
      "  batch 2000 loss: 0.1507522625774145\n",
      "\n",
      "  batch 3000 loss: 0.15024855045974256\n",
      "\n",
      "LOSS train 0.15024855045974256 valid 0.15001972019672394\n",
      "EPOCH 10:\n",
      "  batch 1000 loss: 0.15147010277211667\n",
      "\n",
      "  batch 2000 loss: 0.14965546514093875\n",
      "\n",
      "  batch 3000 loss: 0.14954680322110653\n",
      "\n",
      "LOSS train 0.14954680322110653 valid 0.14639541506767273\n",
      "EPOCH 11:\n",
      "  batch 1000 loss: 0.14956180280447007\n",
      "\n",
      "  batch 2000 loss: 0.14883334252238273\n",
      "\n",
      "  batch 3000 loss: 0.1490426695868373\n",
      "\n",
      "LOSS train 0.1490426695868373 valid 0.1468742936849594\n",
      "EPOCH 12:\n",
      "  batch 1000 loss: 0.14915333583950996\n",
      "\n",
      "  batch 2000 loss: 0.14858704616874457\n",
      "\n",
      "  batch 3000 loss: 0.14850716838240624\n",
      "\n",
      "LOSS train 0.14850716838240624 valid 0.15077702701091766\n",
      "EPOCH 13:\n",
      "  batch 1000 loss: 0.14812806187570096\n",
      "\n",
      "  batch 2000 loss: 0.1481176867559552\n",
      "\n",
      "  batch 3000 loss: 0.1478554241731763\n",
      "\n",
      "LOSS train 0.1478554241731763 valid 0.14423026144504547\n",
      "EPOCH 14:\n",
      "  batch 1000 loss: 0.14827702356129885\n",
      "\n",
      "  batch 2000 loss: 0.14760669689625502\n",
      "\n",
      "  batch 3000 loss: 0.14857257969677448\n",
      "\n",
      "LOSS train 0.14857257969677448 valid 0.14823678135871887\n",
      "EPOCH 15:\n",
      "  batch 1000 loss: 0.14821531096100807\n",
      "\n",
      "  batch 2000 loss: 0.14754844821244478\n",
      "\n",
      "  batch 3000 loss: 0.14838969147205353\n",
      "\n",
      "LOSS train 0.14838969147205353 valid 0.14602409303188324\n",
      "EPOCH 16:\n",
      "  batch 1000 loss: 0.14738382536172867\n",
      "\n",
      "  batch 2000 loss: 0.1475910385027528\n",
      "\n",
      "  batch 3000 loss: 0.1474305757880211\n",
      "\n",
      "LOSS train 0.1474305757880211 valid 0.14383141696453094\n",
      "EPOCH 17:\n",
      "  batch 1000 loss: 0.1482259024530649\n",
      "\n",
      "  batch 2000 loss: 0.14778232371807098\n",
      "\n",
      "  batch 3000 loss: 0.148193995885551\n",
      "\n",
      "LOSS train 0.148193995885551 valid 0.14384374022483826\n",
      "EPOCH 18:\n",
      "  batch 1000 loss: 0.14690996117889882\n",
      "\n",
      "  batch 2000 loss: 0.14685247582942249\n",
      "\n",
      "  batch 3000 loss: 0.14642238504439592\n",
      "\n",
      "LOSS train 0.14642238504439592 valid 0.1450244039297104\n",
      "EPOCH 19:\n",
      "  batch 1000 loss: 0.14655247240513564\n",
      "\n",
      "  batch 2000 loss: 0.14715931163728238\n",
      "\n",
      "  batch 3000 loss: 0.14730651462823152\n",
      "\n",
      "LOSS train 0.14730651462823152 valid 0.14396169781684875\n",
      "EPOCH 20:\n",
      "  batch 1000 loss: 0.14745310016721486\n",
      "\n",
      "  batch 2000 loss: 0.1478474303856492\n",
      "\n",
      "  batch 3000 loss: 0.14691900994628668\n",
      "\n",
      "LOSS train 0.14691900994628668 valid 0.1442803144454956\n",
      "EPOCH 21:\n",
      "  batch 1000 loss: 0.14528283805400133\n",
      "\n",
      "  batch 2000 loss: 0.1454841286316514\n",
      "\n",
      "  batch 3000 loss: 0.1444681765884161\n",
      "\n",
      "LOSS train 0.1444681765884161 valid 0.14399667084217072\n",
      "EPOCH 22:\n",
      "  batch 1000 loss: 0.1459469539746642\n",
      "\n",
      "  batch 2000 loss: 0.1457912690639496\n",
      "\n",
      "  batch 3000 loss: 0.14609454993903637\n",
      "\n",
      "LOSS train 0.14609454993903637 valid 0.1454855054616928\n",
      "EPOCH 23:\n",
      "  batch 1000 loss: 0.1451892054826021\n",
      "\n",
      "  batch 2000 loss: 0.1455541527494788\n",
      "\n",
      "  batch 3000 loss: 0.14611599738150835\n",
      "\n",
      "LOSS train 0.14611599738150835 valid 0.1434634029865265\n",
      "EPOCH 24:\n",
      "  batch 1000 loss: 0.14487375697493554\n",
      "\n",
      "  batch 2000 loss: 0.1463452147692442\n",
      "\n",
      "  batch 3000 loss: 0.14539637192338706\n",
      "\n",
      "LOSS train 0.14539637192338706 valid 0.14278700947761536\n",
      "EPOCH 25:\n",
      "  batch 1000 loss: 0.14576555973291397\n",
      "\n",
      "  batch 2000 loss: 0.14642415350675583\n",
      "\n",
      "  batch 3000 loss: 0.14582805675268173\n",
      "\n",
      "LOSS train 0.14582805675268173 valid 0.1458396166563034\n",
      "EPOCH 26:\n",
      "  batch 1000 loss: 0.1458529879376292\n",
      "\n",
      "  batch 2000 loss: 0.1461459088921547\n",
      "\n",
      "  batch 3000 loss: 0.14662284146994353\n",
      "\n",
      "LOSS train 0.14662284146994353 valid 0.14449718594551086\n",
      "EPOCH 27:\n",
      "  batch 1000 loss: 0.14477913409471513\n",
      "\n",
      "  batch 2000 loss: 0.14578573048114776\n",
      "\n",
      "  batch 3000 loss: 0.14529336754232644\n",
      "\n",
      "LOSS train 0.14529336754232644 valid 0.1568436622619629\n",
      "EPOCH 28:\n",
      "  batch 1000 loss: 0.14622864335030317\n",
      "\n",
      "  batch 2000 loss: 0.14472942344844342\n",
      "\n",
      "  batch 3000 loss: 0.14551602109521627\n",
      "\n",
      "LOSS train 0.14551602109521627 valid 0.14646898210048676\n",
      "EPOCH 29:\n",
      "  batch 1000 loss: 0.14440360546857117\n",
      "\n",
      "  batch 2000 loss: 0.14363748779147864\n",
      "\n",
      "  batch 3000 loss: 0.14346717243641616\n",
      "\n",
      "LOSS train 0.14346717243641616 valid 0.14268754422664642\n",
      "EPOCH 30:\n",
      "  batch 1000 loss: 0.14368503497540952\n",
      "\n",
      "  batch 2000 loss: 0.1439943417981267\n",
      "\n",
      "  batch 3000 loss: 0.14409010708332062\n",
      "\n",
      "LOSS train 0.14409010708332062 valid 0.1435891091823578\n",
      "EPOCH 31:\n",
      "  batch 1000 loss: 0.14386620727926494\n",
      "\n",
      "  batch 2000 loss: 0.14473015742748976\n",
      "\n",
      "  batch 3000 loss: 0.14449827525764702\n",
      "\n",
      "LOSS train 0.14449827525764702 valid 0.14213548600673676\n",
      "EPOCH 32:\n",
      "  batch 1000 loss: 0.14419066648930312\n",
      "\n",
      "  batch 2000 loss: 0.14481831456720828\n",
      "\n",
      "  batch 3000 loss: 0.1453164647370577\n",
      "\n",
      "LOSS train 0.1453164647370577 valid 0.14479772746562958\n",
      "EPOCH 33:\n",
      "  batch 1000 loss: 0.145696569994092\n",
      "\n",
      "  batch 2000 loss: 0.1449499561712146\n",
      "\n",
      "  batch 3000 loss: 0.14539869913458825\n",
      "\n",
      "LOSS train 0.14539869913458825 valid 0.14248134195804596\n",
      "EPOCH 34:\n",
      "  batch 1000 loss: 0.1451160149499774\n",
      "\n",
      "  batch 2000 loss: 0.1450487250611186\n",
      "\n",
      "  batch 3000 loss: 0.1452864481061697\n",
      "\n",
      "LOSS train 0.1452864481061697 valid 0.14140506088733673\n",
      "EPOCH 35:\n",
      "  batch 1000 loss: 0.14607773399353027\n",
      "\n",
      "  batch 2000 loss: 0.14621000161767006\n",
      "\n",
      "  batch 3000 loss: 0.14469571606069803\n",
      "\n",
      "LOSS train 0.14469571606069803 valid 0.143523171544075\n",
      "EPOCH 36:\n",
      "  batch 1000 loss: 0.1463242340683937\n",
      "\n",
      "  batch 2000 loss: 0.14711599493026734\n",
      "\n",
      "  batch 3000 loss: 0.14981360199302435\n",
      "\n",
      "LOSS train 0.14981360199302435 valid 0.15055228769779205\n",
      "EPOCH 37:\n",
      "  batch 1000 loss: 0.1495129450559616\n",
      "\n",
      "  batch 2000 loss: 0.1512822661921382\n",
      "\n",
      "  batch 3000 loss: 0.14925931224226952\n",
      "\n",
      "LOSS train 0.14925931224226952 valid 0.1450156420469284\n",
      "EPOCH 38:\n",
      "  batch 1000 loss: 0.14929150050878526\n",
      "\n",
      "  batch 2000 loss: 0.14956214959919453\n",
      "\n",
      "  batch 3000 loss: 0.14996095524728298\n",
      "\n",
      "LOSS train 0.14996095524728298 valid 0.1474759876728058\n",
      "EPOCH 39:\n",
      "  batch 1000 loss: 0.14719975439459085\n",
      "\n",
      "  batch 2000 loss: 0.14894768293201924\n",
      "\n",
      "  batch 3000 loss: 0.14794249569624662\n",
      "\n",
      "LOSS train 0.14794249569624662 valid 0.14759059250354767\n",
      "EPOCH 40:\n",
      "  batch 1000 loss: 0.1494584506005049\n",
      "\n",
      "  batch 2000 loss: 0.1495696962028742\n",
      "\n",
      "  batch 3000 loss: 0.15193374021351339\n",
      "\n",
      "LOSS train 0.15193374021351339 valid 0.1540570855140686\n",
      "EPOCH 41:\n",
      "  batch 1000 loss: 0.15504222758859396\n",
      "\n",
      "  batch 2000 loss: 0.15947687176615\n",
      "\n",
      "  batch 3000 loss: 0.16565482388436795\n",
      "\n",
      "LOSS train 0.16565482388436795 valid 0.1618528962135315\n",
      "EPOCH 42:\n",
      "  batch 1000 loss: 0.17321969846636057\n",
      "\n",
      "  batch 2000 loss: 0.1784594279676676\n",
      "\n",
      "  batch 3000 loss: 0.18373638485372065\n",
      "\n",
      "LOSS train 0.18373638485372065 valid 0.1848534345626831\n",
      "EPOCH 43:\n",
      "  batch 1000 loss: 0.18205730991065502\n",
      "\n",
      "  batch 2000 loss: 0.18981804883480072\n",
      "\n",
      "  batch 3000 loss: 0.1952021853774786\n",
      "\n",
      "LOSS train 0.1952021853774786 valid 0.19682511687278748\n",
      "EPOCH 44:\n",
      "  batch 1000 loss: 0.20436595955491066\n",
      "\n",
      "  batch 2000 loss: 0.21801740899682046\n",
      "\n",
      "  batch 3000 loss: 0.23395041570067407\n",
      "\n",
      "LOSS train 0.23395041570067407 valid 0.24619263410568237\n",
      "EPOCH 45:\n",
      "  batch 1000 loss: 0.24458960047364234\n",
      "\n",
      "  batch 2000 loss: 0.24526703131198882\n",
      "\n",
      "  batch 3000 loss: 0.2425691041648388\n",
      "\n",
      "LOSS train 0.2425691041648388 valid 0.2691207230091095\n",
      "EPOCH 46:\n",
      "  batch 1000 loss: 0.24287821462750434\n",
      "\n",
      "  batch 2000 loss: 0.24190588857233525\n",
      "\n",
      "  batch 3000 loss: 0.24085734750330448\n",
      "\n",
      "LOSS train 0.24085734750330448 valid 0.22738006711006165\n",
      "EPOCH 47:\n",
      "  batch 1000 loss: 0.23379692655801773\n",
      "\n",
      "  batch 2000 loss: 0.2358550262004137\n",
      "\n",
      "  batch 3000 loss: 0.23499879789352418\n",
      "\n",
      "LOSS train 0.23499879789352418 valid 0.2354292869567871\n",
      "EPOCH 48:\n",
      "  batch 1000 loss: 0.240643105879426\n",
      "\n",
      "  batch 2000 loss: 0.24147295431792737\n",
      "\n",
      "  batch 3000 loss: 0.2449962136745453\n",
      "\n",
      "LOSS train 0.2449962136745453 valid 0.2382894605398178\n",
      "EPOCH 49:\n",
      "  batch 1000 loss: 0.2476432423144579\n",
      "\n",
      "  batch 2000 loss: 0.2540374436825514\n",
      "\n",
      "  batch 3000 loss: 0.25847821670770643\n",
      "\n",
      "LOSS train 0.25847821670770643 valid 0.25139129161834717\n",
      "EPOCH 50:\n",
      "  batch 1000 loss: 0.2665121928155422\n",
      "\n",
      "  batch 2000 loss: 0.2715402517914772\n",
      "\n",
      "  batch 3000 loss: 0.27806366530060767\n",
      "\n",
      "LOSS train 0.27806366530060767 valid 0.28472191095352173\n",
      "Training completed. Model available to use\n"
     ]
    }
   ],
   "source": [
    "train(epochs=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
