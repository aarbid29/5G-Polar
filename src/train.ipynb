{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "589f41f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import PolarDecDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from models.wrappers.mamba_32bits import MambaPolarDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d85178a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 32\n",
    "CONFIG_NO = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e6fb613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78f1e95",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36e1bd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PolarDecDataset(snr_db=10, num_samples=100000, seq_length=N)\n",
    "test_set = PolarDecDataset(snr_db=10, num_samples=3200, seq_length=N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdad0b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset, batch_size = 32)\n",
    "test_dataloader = DataLoader(test_set, batch_size = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbbd230",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17a2a925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MambaPolarDecoder(\n",
       "  (discrete_embedding): Embedding(2, 32)\n",
       "  (linear_embedding1): Linear(in_features=1, out_features=32, bias=True)\n",
       "  (linear_input_layer): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (encoder_layers): ModuleList(\n",
       "    (0): BiMambaEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-7): 8 x BiMambaBlock(\n",
       "          (pre_ln_f): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (mamba_f): Mamba(\n",
       "            (in_proj): Linear(in_features=32, out_features=128, bias=False)\n",
       "            (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)\n",
       "            (act): SiLU()\n",
       "            (x_proj): Linear(in_features=64, out_features=34, bias=False)\n",
       "            (dt_proj): Linear(in_features=2, out_features=64, bias=True)\n",
       "            (out_proj): Linear(in_features=64, out_features=32, bias=False)\n",
       "          )\n",
       "          (post_ln_f): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (ffn_f): Sequential(\n",
       "            (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=128, out_features=32, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (pre_ln_r): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_ln_r): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (mamba_r): Mamba(\n",
       "            (in_proj): Linear(in_features=32, out_features=128, bias=False)\n",
       "            (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)\n",
       "            (act): SiLU()\n",
       "            (x_proj): Linear(in_features=64, out_features=34, bias=False)\n",
       "            (dt_proj): Linear(in_features=2, out_features=64, bias=True)\n",
       "            (out_proj): Linear(in_features=64, out_features=32, bias=False)\n",
       "          )\n",
       "          (ffn_r): Sequential(\n",
       "            (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=128, out_features=32, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "  (post_norms): ModuleList(\n",
       "    (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (final_proj_layer): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MambaPolarDecoder(\n",
    "    d_model=32,\n",
    "    num_layer_encoder=1,\n",
    "    num_layers_bimamba_block=8,\n",
    "    seq_len=N,\n",
    "    d_state=16,\n",
    "    d_conv=4,\n",
    "    expand=2\n",
    ").to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81b5ae3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run/Change if loading an old model only\n",
    "\n",
    "model_path = \"./checkpoints/config_3/model_epoch_29.pt\"\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "\n",
    "state_dict = checkpoint.get(\"model_state_dict\", checkpoint.get(\"state_dict\", checkpoint))\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f20a282",
   "metadata": {},
   "source": [
    "## Minor modification to the Loss Function: Calculates loss only at non frozen positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05e72cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(frozen_bit_prior, target_vector, predicted_vector,  reliable_only=False):\n",
    "    \"\"\"\n",
    "    frozen_bit_prior: tensor of shape (seq_len,) with 1 for frozen, 0 for message bits\n",
    "    target_vector: tensor of shape (seq_len,)\n",
    "    predicted_vector: tensor of shape (seq_len,)\n",
    "    loss_fn: PyTorch loss function\n",
    "    \"\"\"\n",
    "\n",
    "    if reliable_only: \n",
    "     mask = (frozen_bit_prior != 1) \n",
    "     target_vector = target_vector[mask]\n",
    "     predicted_vector = predicted_vector[mask]\n",
    "\n",
    "    #print(\"target vector:\" ,target_vector[:32], \"\\n\")\n",
    "    #print(\"pred vector:\" ,predicted_vector[:32])\n",
    "\n",
    "   # print(f\"Length of reliable bits: {len(reliable_target)}\")\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    return loss_fn(predicted_vector, target_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808bd9a5",
   "metadata": {},
   "source": [
    "## Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5dc9380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted bits:10110100100011101111000011100000\n",
      "\n",
      "Actual bits: 10000010100011101111000011100000\n",
      "\n",
      "Loss: 0.4178037941455841\n"
     ]
    }
   ],
   "source": [
    "llr, frozen_tensor, snr_tensor, target_tensor= next(iter(train_dataloader))\n",
    "ip1 = llr.float().to(device)\n",
    "ip2 = frozen_tensor.int().to(device)\n",
    "ip3 = snr_tensor.float().to(device)\n",
    "\n",
    "predicted = model(ip1, ip2, ip3) #works\n",
    "\n",
    "loss = calculate_loss(ip2, target_tensor.to(device), predicted.to(device)) #works\n",
    "\n",
    "#print(f\"Channel Observation Vector: {channel_tensor}\\n\\n\")\n",
    "\n",
    "\n",
    "#print(f\"Channel Observation Vector: {ip1.shape}\\nFrozen Tensor: {ip2.shape}\\n\")\n",
    "#print(f\"Predicted Channel Input Vector(logits): {predicted.shape}\\n\\n\")\n",
    "\n",
    "#print(f\"Predicted (sigmoid): {torch.sigmoid(predicted)}\\n\\n\")\n",
    "pred = (torch.sigmoid(predicted) > 0.5).long()[0]\n",
    "\n",
    "print(f\"Predicted bits:{''.join(map(str, pred.cpu().tolist()))}\\n\")\n",
    "print(f\"Actual bits: {''.join(str(int(i)) for i in target_tensor[0])}\\n\")\n",
    "print(f\"Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69534924",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=0)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', \n",
    "                                                        factor=0.5, patience=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32f8dbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index):\n",
    "\n",
    "    running_loss = 0\n",
    "    last_loss = 0\n",
    "\n",
    "\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        channel_tensor, frozen_tensor, snr_tensor, target_tensor = data\n",
    "        ip1 = channel_tensor.float().to(device)\n",
    "        ip2 = frozen_tensor.int().to(device)\n",
    "        ip3 = snr_tensor.float().to(device)\n",
    "        op = target_tensor.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(ip1,ip2 ,ip3 ).to(device)\n",
    "\n",
    "    #    B, L, C = outputs.shape\n",
    "    #    output_logits = outputs.view(B*L, C).to(device)\n",
    "    #    target_flattened = shifted.view(B*L).to(device).long()\n",
    "\n",
    "\n",
    "    #    loss = loss_fn(output_logits, target_flattened)\n",
    "        \n",
    "        loss = calculate_loss(ip2, op, outputs)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i%1000 == 999:\n",
    "            last_loss = running_loss/1000\n",
    "            print('  batch {} loss: {}\\n'.format(i + 1, last_loss))\n",
    "          #  print(f\"Predictions look currently like: {outputs[:32]}\\n\\n\")\n",
    "            running_loss = 0.\n",
    "    return last_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f26fb8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train( epochs=50):\n",
    "\n",
    "    best_vloss = 1_000_000.\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "     print('EPOCH {}:'.format(epoch + 1))\n",
    " \n",
    "   \n",
    "     model.train(True)\n",
    "     avg_loss = train_one_epoch(epoch)\n",
    "\n",
    "\n",
    "     running_vloss = 0.0\n",
    "    \n",
    "     model.eval()\n",
    "\n",
    "   \n",
    "     with torch.no_grad():\n",
    "        for i, vdata in enumerate(test_dataloader):\n",
    "            vchannel_tensor, vfrozen_tensor, vsnr_tensor, vtarget_tensor = vdata\n",
    "            voutputs = model(vchannel_tensor.float().to(device), vfrozen_tensor.int().to(device), vsnr_tensor.float().to(device))\n",
    "          #  B, L, C = voutputs.shape\n",
    "          #  vloss = loss_fn(voutputs.view(B*L, C).to(device), vlabels.view(B*L).to(device))\n",
    "            \n",
    "            vloss = calculate_loss(vfrozen_tensor.to(device), vtarget_tensor.to(device), voutputs.to(device))\n",
    "            running_vloss += vloss\n",
    "\n",
    "     avg_vloss = running_vloss / (i + 1)\n",
    "     print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    \n",
    "     scheduler.step(avg_vloss)\n",
    "\n",
    "    \n",
    "     if avg_vloss < best_vloss:\n",
    "      \n",
    "    \n",
    "      best_vloss = avg_vloss\n",
    "      model_path = f'./checkpoints/config_{CONFIG_NO}/model_epoch_{epoch}.pt'\n",
    "      torch.save({\n",
    "         \"comments\": \"Removed the snr as input entirely. (even if used in future, use as snr linear, not in db)\",\n",
    "    'model_config': {\n",
    "        \"d_model\": model.d_model,\n",
    "        \"num_layer_encoder\": model.num_layer_encoder,\n",
    "        \"num_layers_bimamba_block\": model.num_layers_bimamba_block,\n",
    "        \"seq_len\": model.seq_len,\n",
    "        \"d_state\": model.d_state,\n",
    "        \"d_conv\": model.d_conv,\n",
    "        \"expand\": model.expand,\n",
    "    },\n",
    "    'epoch': epoch + 1,\n",
    "    'train_loss': avg_loss,\n",
    "    'val_loss': avg_vloss,\n",
    "    'state_dict': model.state_dict()\n",
    "}, model_path)\n",
    "\n",
    "\n",
    "     \n",
    "    print(\"Training completed. Model available to use\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d589f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 1000 loss: 0.16876117210835218\n",
      "\n",
      "  batch 2000 loss: 0.13451092935353517\n",
      "\n",
      "  batch 3000 loss: 0.12998631876707076\n",
      "\n",
      "LOSS train 0.12998631876707076 valid 0.13152478635311127\n",
      "EPOCH 2:\n",
      "  batch 1000 loss: 0.1293222303763032\n",
      "\n",
      "  batch 2000 loss: 0.12926761538535356\n",
      "\n",
      "  batch 3000 loss: 0.12875927066802978\n",
      "\n",
      "LOSS train 0.12875927066802978 valid 0.12942537665367126\n",
      "EPOCH 3:\n",
      "  batch 1000 loss: 0.1291515857130289\n",
      "\n",
      "  batch 2000 loss: 0.12859168649464844\n",
      "\n",
      "  batch 3000 loss: 0.12851607444137333\n",
      "\n",
      "LOSS train 0.12851607444137333 valid 0.12577594816684723\n",
      "EPOCH 4:\n",
      "  batch 1000 loss: 0.12914862057566642\n",
      "\n",
      "  batch 2000 loss: 0.1288262109979987\n",
      "\n",
      "  batch 3000 loss: 0.12876178611814976\n",
      "\n",
      "LOSS train 0.12876178611814976 valid 0.13512858748435974\n",
      "EPOCH 5:\n",
      "  batch 1000 loss: 0.1296358618363738\n",
      "\n",
      "  batch 2000 loss: 0.1283669722750783\n",
      "\n",
      "  batch 3000 loss: 0.12912962184101343\n",
      "\n",
      "LOSS train 0.12912962184101343 valid 0.1289234757423401\n",
      "EPOCH 6:\n",
      "  batch 1000 loss: 0.12744715800881387\n",
      "\n",
      "  batch 2000 loss: 0.12946578192710875\n",
      "\n",
      "  batch 3000 loss: 0.12797395531833172\n",
      "\n",
      "LOSS train 0.12797395531833172 valid 0.1260184794664383\n",
      "EPOCH 7:\n",
      "  batch 1000 loss: 0.12872231980413198\n",
      "\n",
      "  batch 2000 loss: 0.12813233864307402\n",
      "\n",
      "  batch 3000 loss: 0.1286590363755822\n",
      "\n",
      "LOSS train 0.1286590363755822 valid 0.12973475456237793\n",
      "EPOCH 8:\n",
      "  batch 1000 loss: 0.12758432174474002\n",
      "\n",
      "  batch 2000 loss: 0.12812852121889592\n",
      "\n",
      "  batch 3000 loss: 0.12772926094383003\n",
      "\n",
      "LOSS train 0.12772926094383003 valid 0.1274002194404602\n",
      "EPOCH 9:\n",
      "  batch 1000 loss: 0.12765449358522893\n",
      "\n",
      "  batch 2000 loss: 0.1278576187416911\n",
      "\n",
      "  batch 3000 loss: 0.12717157158255576\n",
      "\n",
      "LOSS train 0.12717157158255576 valid 0.1265917271375656\n",
      "EPOCH 10:\n",
      "  batch 1000 loss: 0.12681002892553805\n",
      "\n",
      "  batch 2000 loss: 0.12691721645742654\n",
      "\n",
      "  batch 3000 loss: 0.12701898131519557\n",
      "\n",
      "LOSS train 0.12701898131519557 valid 0.12586775422096252\n",
      "EPOCH 11:\n",
      "  batch 1000 loss: 0.1209678065776825\n",
      "\n",
      "  batch 2000 loss: 0.12036764719337224\n",
      "\n",
      "  batch 3000 loss: 0.12081606964766979\n",
      "\n",
      "LOSS train 0.12081606964766979 valid 0.11853665858507156\n",
      "EPOCH 12:\n",
      "  batch 1000 loss: 0.11978990366309882\n",
      "\n",
      "  batch 2000 loss: 0.1210022871568799\n",
      "\n",
      "  batch 3000 loss: 0.11965871211886406\n",
      "\n",
      "LOSS train 0.11965871211886406 valid 0.11877868324518204\n",
      "EPOCH 13:\n",
      "  batch 1000 loss: 0.11965478979796171\n",
      "\n",
      "  batch 2000 loss: 0.12005037451535464\n",
      "\n",
      "  batch 3000 loss: 0.11940065761655569\n",
      "\n",
      "LOSS train 0.11940065761655569 valid 0.12040739506483078\n",
      "EPOCH 14:\n",
      "  batch 1000 loss: 0.12012146434932947\n",
      "\n",
      "  batch 2000 loss: 0.11936912208795547\n",
      "\n",
      "  batch 3000 loss: 0.120002715498209\n",
      "\n",
      "LOSS train 0.120002715498209 valid 0.12114842236042023\n",
      "EPOCH 15:\n",
      "  batch 1000 loss: 0.12015377708524466\n",
      "\n",
      "  batch 2000 loss: 0.12030885671079158\n",
      "\n",
      "  batch 3000 loss: 0.120428720459342\n",
      "\n",
      "LOSS train 0.120428720459342 valid 0.12099148333072662\n",
      "EPOCH 16:\n",
      "  batch 1000 loss: 0.12005344448238611\n",
      "\n",
      "  batch 2000 loss: 0.11974173092842103\n",
      "\n",
      "  batch 3000 loss: 0.1202245699763298\n",
      "\n",
      "LOSS train 0.1202245699763298 valid 0.12003450840711594\n",
      "EPOCH 17:\n",
      "  batch 1000 loss: 0.11924842681735753\n",
      "\n",
      "  batch 2000 loss: 0.1198533312380314\n",
      "\n",
      "  batch 3000 loss: 0.11989895401149989\n",
      "\n",
      "LOSS train 0.11989895401149989 valid 0.11948242038488388\n",
      "EPOCH 18:\n",
      "  batch 1000 loss: 0.12007400173693895\n",
      "\n",
      "  batch 2000 loss: 0.1199471624866128\n",
      "\n",
      "  batch 3000 loss: 0.12055930417776108\n",
      "\n",
      "LOSS train 0.12055930417776108 valid 0.11998007446527481\n",
      "EPOCH 19:\n",
      "  batch 1000 loss: 0.11957908897846938\n",
      "\n",
      "  batch 2000 loss: 0.11964353474229575\n",
      "\n",
      "  batch 3000 loss: 0.12024977938830853\n",
      "\n",
      "LOSS train 0.12024977938830853 valid 0.1191023737192154\n",
      "EPOCH 20:\n",
      "  batch 1000 loss: 0.12012350700050592\n",
      "\n",
      "  batch 2000 loss: 0.12065294723957777\n",
      "\n",
      "  batch 3000 loss: 0.11997867351770401\n",
      "\n",
      "LOSS train 0.11997867351770401 valid 0.12114238739013672\n",
      "EPOCH 21:\n",
      "  batch 1000 loss: 0.12087447728961706\n",
      "\n",
      "  batch 2000 loss: 0.11959367857128382\n",
      "\n",
      "  batch 3000 loss: 0.11963450636714697\n",
      "\n",
      "LOSS train 0.11963450636714697 valid 0.11463265866041183\n",
      "EPOCH 22:\n",
      "  batch 1000 loss: 0.11390301348268986\n",
      "\n",
      "  batch 2000 loss: 0.11291446799784899\n",
      "\n",
      "  batch 3000 loss: 0.11253876635432243\n",
      "\n",
      "LOSS train 0.11253876635432243 valid 0.11473266035318375\n",
      "EPOCH 23:\n",
      "  batch 1000 loss: 0.11242888747900724\n",
      "\n",
      "  batch 2000 loss: 0.1126548484787345\n",
      "\n",
      "  batch 3000 loss: 0.11302987776696682\n",
      "\n",
      "LOSS train 0.11302987776696682 valid 0.11535104364156723\n",
      "EPOCH 24:\n",
      "  batch 1000 loss: 0.1129491808488965\n",
      "\n",
      "  batch 2000 loss: 0.11347397372126579\n",
      "\n",
      "  batch 3000 loss: 0.11289647555351258\n",
      "\n",
      "LOSS train 0.11289647555351258 valid 0.11233243346214294\n",
      "EPOCH 25:\n",
      "  batch 1000 loss: 0.11290327030420304\n",
      "\n",
      "  batch 2000 loss: 0.11264480030536651\n",
      "\n",
      "  batch 3000 loss: 0.1127461318075657\n",
      "\n",
      "LOSS train 0.1127461318075657 valid 0.1115373969078064\n",
      "EPOCH 26:\n",
      "  batch 1000 loss: 0.11290824904292822\n",
      "\n",
      "  batch 2000 loss: 0.11286897336691618\n",
      "\n",
      "  batch 3000 loss: 0.11274828741699457\n",
      "\n",
      "LOSS train 0.11274828741699457 valid 0.11311119794845581\n",
      "EPOCH 27:\n",
      "  batch 1000 loss: 0.11271340367943049\n",
      "\n",
      "  batch 2000 loss: 0.11349120540171861\n",
      "\n",
      "  batch 3000 loss: 0.11272976135462523\n",
      "\n",
      "LOSS train 0.11272976135462523 valid 0.11390873044729233\n",
      "EPOCH 28:\n",
      "  batch 1000 loss: 0.1130053679421544\n",
      "\n",
      "  batch 2000 loss: 0.11295337980985641\n",
      "\n",
      "  batch 3000 loss: 0.11294163995236159\n",
      "\n",
      "LOSS train 0.11294163995236159 valid 0.11311905831098557\n",
      "EPOCH 29:\n",
      "  batch 1000 loss: 0.11299308958649636\n",
      "\n",
      "  batch 2000 loss: 0.11267177025973797\n",
      "\n",
      "  batch 3000 loss: 0.11313665433973073\n",
      "\n",
      "LOSS train 0.11313665433973073 valid 0.11187819391489029\n",
      "EPOCH 30:\n",
      "  batch 1000 loss: 0.11288255542516708\n",
      "\n",
      "  batch 2000 loss: 0.11335041446983814\n",
      "\n",
      "  batch 3000 loss: 0.1137751340791583\n",
      "\n",
      "LOSS train 0.1137751340791583 valid 0.1140521764755249\n",
      "EPOCH 31:\n",
      "  batch 1000 loss: 0.11248428774625063\n",
      "\n",
      "  batch 2000 loss: 0.11266522148251533\n",
      "\n",
      "  batch 3000 loss: 0.11299030072987079\n",
      "\n",
      "LOSS train 0.11299030072987079 valid 0.11052095144987106\n",
      "EPOCH 32:\n",
      "  batch 1000 loss: 0.11308723309636116\n",
      "\n",
      "  batch 2000 loss: 0.11285218414664268\n",
      "\n",
      "  batch 3000 loss: 0.11289961884170771\n",
      "\n",
      "LOSS train 0.11289961884170771 valid 0.1125732809305191\n",
      "EPOCH 33:\n",
      "  batch 1000 loss: 0.11292744813114405\n",
      "\n",
      "  batch 2000 loss: 0.11268110541254282\n",
      "\n",
      "  batch 3000 loss: 0.1130384728461504\n",
      "\n",
      "LOSS train 0.1130384728461504 valid 0.11350001394748688\n",
      "EPOCH 34:\n",
      "  batch 1000 loss: 0.11239312215149402\n",
      "\n",
      "  batch 2000 loss: 0.11312609727680684\n",
      "\n",
      "  batch 3000 loss: 0.11217148159444332\n",
      "\n",
      "LOSS train 0.11217148159444332 valid 0.11248501390218735\n",
      "EPOCH 35:\n",
      "  batch 1000 loss: 0.11315163550525904\n",
      "\n",
      "  batch 2000 loss: 0.11298067528009414\n",
      "\n",
      "  batch 3000 loss: 0.11321499788761138\n",
      "\n",
      "LOSS train 0.11321499788761138 valid 0.11212632060050964\n",
      "EPOCH 36:\n",
      "  batch 1000 loss: 0.11258350004255771\n",
      "\n",
      "  batch 2000 loss: 0.11247836276888848\n",
      "\n",
      "  batch 3000 loss: 0.11249531964212656\n",
      "\n",
      "LOSS train 0.11249531964212656 valid 0.11300437897443771\n",
      "EPOCH 37:\n",
      "  batch 1000 loss: 0.11336064422130585\n",
      "\n",
      "  batch 2000 loss: 0.11242498130351306\n",
      "\n",
      "  batch 3000 loss: 0.11198475784808397\n",
      "\n",
      "LOSS train 0.11198475784808397 valid 0.11352673918008804\n",
      "EPOCH 38:\n",
      "  batch 1000 loss: 0.1129272775426507\n",
      "\n",
      "  batch 2000 loss: 0.11358260236680508\n",
      "\n",
      "  batch 3000 loss: 0.11258125349879265\n",
      "\n",
      "LOSS train 0.11258125349879265 valid 0.1106758862733841\n",
      "EPOCH 39:\n",
      "  batch 1000 loss: 0.11294691438227891\n",
      "\n",
      "  batch 2000 loss: 0.11296657055616378\n",
      "\n",
      "  batch 3000 loss: 0.1134814425483346\n",
      "\n",
      "LOSS train 0.1134814425483346 valid 0.1115727424621582\n",
      "EPOCH 40:\n",
      "  batch 1000 loss: 0.11287706754356623\n",
      "\n",
      "  batch 2000 loss: 0.11308351346850395\n",
      "\n",
      "  batch 3000 loss: 0.11236074597388505\n",
      "\n",
      "LOSS train 0.11236074597388505 valid 0.11088727414608002\n",
      "EPOCH 41:\n",
      "  batch 1000 loss: 0.11293410127609968\n",
      "\n",
      "  batch 2000 loss: 0.11307471480220556\n",
      "\n",
      "  batch 3000 loss: 0.11323442444577814\n",
      "\n",
      "LOSS train 0.11323442444577814 valid 0.1101871207356453\n",
      "EPOCH 42:\n",
      "  batch 1000 loss: 0.1121987211406231\n",
      "\n",
      "  batch 2000 loss: 0.11317466270923615\n",
      "\n",
      "  batch 3000 loss: 0.11257836834341288\n",
      "\n",
      "LOSS train 0.11257836834341288 valid 0.11292970180511475\n",
      "EPOCH 43:\n",
      "  batch 1000 loss: 0.11283595213294029\n",
      "\n",
      "  batch 2000 loss: 0.11278241365402937\n",
      "\n",
      "  batch 3000 loss: 0.1126066953241825\n",
      "\n",
      "LOSS train 0.1126066953241825 valid 0.1148374080657959\n",
      "EPOCH 44:\n",
      "  batch 1000 loss: 0.11340955045819283\n",
      "\n",
      "  batch 2000 loss: 0.11263114800304175\n",
      "\n",
      "  batch 3000 loss: 0.11330174065381289\n",
      "\n",
      "LOSS train 0.11330174065381289 valid 0.11105552315711975\n",
      "EPOCH 45:\n",
      "  batch 1000 loss: 0.1130299245417118\n",
      "\n",
      "  batch 2000 loss: 0.11372296748310327\n",
      "\n",
      "  batch 3000 loss: 0.11310599462687969\n",
      "\n",
      "LOSS train 0.11310599462687969 valid 0.11239470541477203\n",
      "EPOCH 46:\n",
      "  batch 1000 loss: 0.11260042319446802\n",
      "\n",
      "  batch 2000 loss: 0.11256860662251711\n",
      "\n",
      "  batch 3000 loss: 0.11241207394748925\n",
      "\n",
      "LOSS train 0.11241207394748925 valid 0.11335846781730652\n",
      "EPOCH 47:\n",
      "  batch 1000 loss: 0.1130678034722805\n",
      "\n",
      "  batch 2000 loss: 0.11278240637481213\n",
      "\n",
      "  batch 3000 loss: 0.11254945262521505\n",
      "\n",
      "LOSS train 0.11254945262521505 valid 0.11132070422172546\n",
      "EPOCH 48:\n",
      "  batch 1000 loss: 0.11240546455234289\n",
      "\n",
      "  batch 2000 loss: 0.1132107585966587\n",
      "\n",
      "  batch 3000 loss: 0.11349404665827752\n",
      "\n",
      "LOSS train 0.11349404665827752 valid 0.11494389176368713\n",
      "EPOCH 49:\n",
      "  batch 1000 loss: 0.11260942765697837\n",
      "\n",
      "  batch 2000 loss: 0.11225774090737105\n",
      "\n",
      "  batch 3000 loss: 0.11253157467395067\n",
      "\n",
      "LOSS train 0.11253157467395067 valid 0.1119668111205101\n",
      "EPOCH 50:\n",
      "  batch 1000 loss: 0.1132563284933567\n",
      "\n",
      "  batch 2000 loss: 0.11276400419324636\n",
      "\n",
      "  batch 3000 loss: 0.112957804992795\n",
      "\n",
      "LOSS train 0.112957804992795 valid 0.11201204359531403\n",
      "Training completed. Model available to use\n"
     ]
    }
   ],
   "source": [
    "train(epochs=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
