{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "589f41f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anjal/Desktop/Code/mamba-seq-pred/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dataset import PolarDecDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from models.wrappers.mamba_32bits import MambaPolarDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85178a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 32\n",
    "CONFIG_NO = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e6fb613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78f1e95",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36e1bd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PolarDecDataset(snr_db=10, num_samples=1000000)\n",
    "test_set = PolarDecDataset(snr_db=10, num_samples=1000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdad0b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset, batch_size = 32)\n",
    "test_dataloader = DataLoader(test_set, batch_size = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbbd230",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17a2a925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MambaPolarDecoder(\n",
       "  (discrete_embedding): Embedding(2, 64)\n",
       "  (linear_embedding1): Linear(in_features=1, out_features=64, bias=True)\n",
       "  (linear_embedding2): Linear(in_features=1, out_features=64, bias=True)\n",
       "  (encoder_layers): ModuleList(\n",
       "    (0): BiMambaEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x BiMambaBlock(\n",
       "          (pre_ln_f): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (mamba_f): Mamba(\n",
       "            (in_proj): Linear(in_features=64, out_features=256, bias=False)\n",
       "            (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)\n",
       "            (act): SiLU()\n",
       "            (x_proj): Linear(in_features=128, out_features=68, bias=False)\n",
       "            (dt_proj): Linear(in_features=4, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=64, bias=False)\n",
       "          )\n",
       "          (post_ln_f): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (ffn_f): Sequential(\n",
       "            (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=256, out_features=64, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (pre_ln_r): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_ln_r): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (mamba_r): Mamba(\n",
       "            (in_proj): Linear(in_features=64, out_features=256, bias=False)\n",
       "            (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)\n",
       "            (act): SiLU()\n",
       "            (x_proj): Linear(in_features=128, out_features=68, bias=False)\n",
       "            (dt_proj): Linear(in_features=4, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=64, bias=False)\n",
       "          )\n",
       "          (ffn_r): Sequential(\n",
       "            (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=256, out_features=64, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (final_proj_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MambaPolarDecoder(\n",
    "    d_model=64,\n",
    "    num_layer_encoder=1,\n",
    "    num_layers_bimamba_block=32,\n",
    "    seq_len=N,\n",
    "    d_state=32,\n",
    "    d_conv=4,\n",
    "    expand=2\n",
    ").to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f20a282",
   "metadata": {},
   "source": [
    "## Minor modification to the Loss Function: Calculates loss only at non frozen positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05e72cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss_for_reliable_bits_only(frozen_bit_prior, target_vector, predicted_vector, loss_fn):\n",
    "    \"\"\"\n",
    "    frozen_bit_prior: tensor of shape (seq_len,) with 1 for frozen, 0 for message bits\n",
    "    target_vector: tensor of shape (seq_len,)\n",
    "    predicted_vector: tensor of shape (seq_len,)\n",
    "    loss_fn: PyTorch loss function\n",
    "    \"\"\"\n",
    "    mask = (frozen_bit_prior != 1) \n",
    "    \n",
    "   \n",
    "    reliable_target = target_vector[mask]\n",
    "    reliable_predicted = predicted_vector[mask]\n",
    "\n",
    "   # print(f\"Length of reliable bits: {len(reliable_target)}\")\n",
    "\n",
    "    return loss_fn(reliable_predicted, reliable_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "230e81dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808bd9a5",
   "metadata": {},
   "source": [
    "## Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d5dc9380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel Observation Vector: torch.Size([32, 32])\n",
      "Frozen Tensor: torch.Size([32, 32])\n",
      "\n",
      "Predicted Channel Input Vector(logits): torch.Size([32, 32])\n",
      "\n",
      "\n",
      "Predicted (sigmoid): tensor([[1.0000e+00, 9.9997e-01, 1.0000e+00,  ..., 9.9986e-01, 9.9977e-01,\n",
      "         1.0000e+00],\n",
      "        [7.0380e-05, 3.9573e-05, 8.5431e-01,  ..., 1.0000e+00, 9.9999e-01,\n",
      "         9.9997e-01],\n",
      "        [9.9998e-01, 1.0000e+00, 9.9948e-01,  ..., 9.9993e-01, 1.0000e+00,\n",
      "         9.9745e-01],\n",
      "        ...,\n",
      "        [5.6786e-03, 2.2419e-04, 9.9998e-01,  ..., 9.9931e-01, 9.9986e-01,\n",
      "         9.9999e-01],\n",
      "        [9.9997e-01, 4.7640e-03, 9.9833e-01,  ..., 9.9836e-01, 9.9982e-01,\n",
      "         9.9991e-01],\n",
      "        [8.9023e-01, 3.7809e-03, 1.0441e-04,  ..., 9.9979e-01, 1.0000e+00,\n",
      "         9.9931e-01]], device='cuda:0', grad_fn=<SigmoidBackward0>)\n",
      "\n",
      "\n",
      "Predicted bits:11110101100111110111111101111111\n",
      "\n",
      "Actual bits: 01000010111000000000000010000000\n",
      "\n",
      "Loss: 6.650700569152832\n"
     ]
    }
   ],
   "source": [
    "llr, frozen_tensor, snr_tensor, target_tensor= next(iter(train_dataloader))\n",
    "ip1 = llr.float().to(device)\n",
    "ip2 = frozen_tensor.int().to(device)\n",
    "ip3 = snr_tensor.float().to(device)\n",
    "\n",
    "predicted = model(ip1, ip2, ip3) #works\n",
    "\n",
    "loss = calculate_loss_for_reliable_bits_only(ip2, target_tensor.to(device), predicted.to(device), loss_fn) #works\n",
    "\n",
    "#print(f\"Channel Observation Vector: {channel_tensor}\\n\\n\")\n",
    "\n",
    "\n",
    "print(f\"Channel Observation Vector: {ip1.shape}\\nFrozen Tensor: {ip2.shape}\\n\")\n",
    "print(f\"Predicted Channel Input Vector(logits): {predicted.shape}\\n\\n\")\n",
    "\n",
    "print(f\"Predicted (sigmoid): {torch.sigmoid(predicted)}\\n\\n\")\n",
    "pred = (torch.sigmoid(predicted) > 0.5).long()[0]\n",
    "\n",
    "print(f\"Predicted bits:{''.join(map(str, pred.cpu().tolist()))}\\n\")\n",
    "print(f\"Actual bits: {''.join(str(int(i)) for i in target_tensor[0])}\\n\")\n",
    "print(f\"Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "69534924",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.001)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', \n",
    "                                                        factor=0.5, patience=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "32f8dbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index):\n",
    "\n",
    "    running_loss = 0\n",
    "    last_loss = 0\n",
    "\n",
    "\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        channel_tensor, frozen_tensor, snr_tensor, target_tensor = data\n",
    "        ip1 = channel_tensor.float().to(device)\n",
    "        ip2 = frozen_tensor.int().to(device)\n",
    "        ip3 = snr_tensor.float().to(device)\n",
    "        op = target_tensor.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(ip1,ip2 ,ip3 ).to(device)\n",
    "\n",
    "    #    B, L, C = outputs.shape\n",
    "    #    output_logits = outputs.view(B*L, C).to(device)\n",
    "    #    target_flattened = shifted.view(B*L).to(device).long()\n",
    "\n",
    "\n",
    "    #    loss = loss_fn(output_logits, target_flattened)\n",
    "        \n",
    "        loss = calculate_loss_for_reliable_bits_only(ip2, op, outputs, loss_fn)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i%1000 == 999:\n",
    "            last_loss = running_loss/1000\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            running_loss = 0.\n",
    "    return last_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26fb8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train( epochs=50):\n",
    "\n",
    "    best_vloss = 1_000_000.\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "     print('EPOCH {}:'.format(epoch + 1))\n",
    " \n",
    "   \n",
    "     model.train(True)\n",
    "     avg_loss = train_one_epoch(epoch)\n",
    "\n",
    "\n",
    "     running_vloss = 0.0\n",
    "    \n",
    "     model.eval()\n",
    "\n",
    "   \n",
    "     with torch.no_grad():\n",
    "        for i, vdata in enumerate(test_dataloader):\n",
    "            vchannel_tensor, vfrozen_tensor, vsnr_tensor, vtarget_tensor = vdata\n",
    "            voutputs = model(vchannel_tensor.float().to(device), vfrozen_tensor.int().to(device), vsnr_tensor.float().to(device))\n",
    "          #  B, L, C = voutputs.shape\n",
    "          #  vloss = loss_fn(voutputs.view(B*L, C).to(device), vlabels.view(B*L).to(device))\n",
    "            \n",
    "            vloss = calculate_loss_for_reliable_bits_only(vfrozen_tensor.to(device), vtarget_tensor.to(device), voutputs.to(device), loss_fn)\n",
    "            running_vloss += vloss\n",
    "\n",
    "     avg_vloss = running_vloss / (i + 1)\n",
    "     print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    \n",
    "     scheduler.step(avg_vloss)\n",
    "\n",
    "    \n",
    "     if avg_vloss < best_vloss:\n",
    "      \n",
    "    \n",
    "      best_vloss = avg_vloss\n",
    "      model_path = f'./checkpoints/config_{CONFIG_NO}/model_epoch_{epoch}.pt'\n",
    "      torch.save({\n",
    "    'model_config': {\n",
    "        \"d_model\": model.d_model,\n",
    "        \"num_layer_encoder\": model.num_layer_encoder,\n",
    "        \"num_layers_bimamba_block\": model.num_layers_bimamba_block,\n",
    "        \"seq_len\": model.seq_len,\n",
    "        \"d_state\": model.d_state,\n",
    "        \"d_conv\": model.d_conv,\n",
    "        \"expand\": model.expand,\n",
    "    },\n",
    "    'epoch': epoch + 1,\n",
    "    'train_loss': avg_loss,\n",
    "    'val_loss': avg_vloss,\n",
    "    'state_dict': model.state_dict()\n",
    "}, model_path)\n",
    "\n",
    "\n",
    "     \n",
    "    print(\"Training completed. Model available to use\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6d589f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 1000 loss: 0.5336742365956306\n",
      "  batch 2000 loss: 0.3722417163848877\n",
      "  batch 3000 loss: 0.327930050522089\n",
      "  batch 4000 loss: 0.3215761846899986\n",
      "  batch 5000 loss: 0.31960532972216604\n",
      "  batch 6000 loss: 0.31790875414013864\n",
      "  batch 7000 loss: 0.31538455495238304\n",
      "  batch 8000 loss: 0.3153826283812523\n",
      "  batch 9000 loss: 0.3144086654186249\n",
      "  batch 10000 loss: 0.31567151948809624\n",
      "  batch 11000 loss: 0.31377769938111305\n",
      "  batch 12000 loss: 0.3152754058539867\n",
      "  batch 13000 loss: 0.3129089913368225\n",
      "  batch 14000 loss: 0.31123180663585664\n",
      "  batch 15000 loss: 0.3063631791770458\n",
      "  batch 16000 loss: 0.30533461801707745\n",
      "  batch 17000 loss: 0.3048073563277721\n",
      "  batch 18000 loss: 0.3055128723680973\n",
      "  batch 19000 loss: 0.30529735863208773\n",
      "  batch 20000 loss: 0.3047070904374123\n",
      "  batch 21000 loss: 0.30450183027982713\n",
      "  batch 22000 loss: 0.30586443743109704\n",
      "  batch 23000 loss: 0.3029697656780481\n",
      "  batch 24000 loss: 0.3041873051673174\n",
      "  batch 25000 loss: 0.3047487173229456\n",
      "  batch 26000 loss: 0.30363707695901393\n",
      "  batch 27000 loss: 0.30366587127745154\n",
      "  batch 28000 loss: 0.3045652985870838\n",
      "  batch 29000 loss: 0.30561488354206084\n",
      "  batch 30000 loss: 0.30304418870806693\n",
      "  batch 31000 loss: 0.3032391125559807\n",
      "LOSS train 0.3032391125559807 valid 0.30383560061454773\n",
      "EPOCH 2:\n",
      "  batch 1000 loss: 0.3041723992675543\n",
      "  batch 2000 loss: 0.30416641941666606\n",
      "  batch 3000 loss: 0.3037718770802021\n",
      "  batch 4000 loss: 0.30353053778409955\n",
      "  batch 5000 loss: 0.3034346064776182\n",
      "  batch 6000 loss: 0.3045576418787241\n",
      "  batch 7000 loss: 0.30430007830262185\n",
      "  batch 8000 loss: 0.3041364744305611\n",
      "  batch 9000 loss: 0.30440762588381765\n",
      "  batch 10000 loss: 0.30345299318432806\n",
      "  batch 11000 loss: 0.3037070931792259\n",
      "  batch 12000 loss: 0.30212834326922894\n",
      "  batch 13000 loss: 0.3040667657107115\n",
      "  batch 14000 loss: 0.3038624540120363\n",
      "  batch 15000 loss: 0.30321816000342366\n",
      "  batch 16000 loss: 0.302603265196085\n",
      "  batch 17000 loss: 0.30321239081025125\n",
      "  batch 18000 loss: 0.30183657544851306\n",
      "  batch 19000 loss: 0.2997758102864027\n",
      "  batch 20000 loss: 0.29699938897788525\n",
      "  batch 21000 loss: 0.29659827961027624\n",
      "  batch 22000 loss: 0.2960297131240368\n",
      "  batch 23000 loss: 0.296180641323328\n",
      "  batch 24000 loss: 0.2959466828107834\n",
      "  batch 25000 loss: 0.29398207730054854\n",
      "  batch 26000 loss: 0.29347629816830156\n",
      "  batch 27000 loss: 0.28963083258271216\n",
      "  batch 28000 loss: 0.28781517000496387\n",
      "  batch 29000 loss: 0.28528048449754717\n",
      "  batch 30000 loss: 0.28397160190343856\n",
      "  batch 31000 loss: 0.2835718630701303\n",
      "LOSS train 0.2835718630701303 valid 0.27640828490257263\n",
      "EPOCH 3:\n",
      "  batch 1000 loss: 0.28302908171713353\n",
      "  batch 2000 loss: 0.28038477793335914\n",
      "  batch 3000 loss: 0.2772426660358906\n",
      "  batch 4000 loss: 0.2776438556611538\n",
      "  batch 5000 loss: 0.2761127199083567\n",
      "  batch 6000 loss: 0.27234627625346186\n",
      "  batch 7000 loss: 0.27134446263313294\n",
      "  batch 8000 loss: 0.26909074386954307\n",
      "  batch 9000 loss: 0.2731570055782795\n",
      "  batch 10000 loss: 0.2664600139707327\n",
      "  batch 11000 loss: 0.2727181876152754\n",
      "  batch 12000 loss: 0.2656727137416601\n",
      "  batch 13000 loss: 0.2674631384015083\n",
      "  batch 14000 loss: 0.26818796406686307\n",
      "  batch 15000 loss: 0.26798175571858884\n",
      "  batch 16000 loss: 0.26652673496305945\n",
      "  batch 17000 loss: 0.26556747786700724\n",
      "  batch 18000 loss: 0.26655497680604456\n",
      "  batch 19000 loss: 0.2676972287148237\n",
      "  batch 20000 loss: 0.2715616174638271\n",
      "  batch 21000 loss: 0.2755592038333416\n",
      "  batch 22000 loss: 0.26772526009380815\n",
      "  batch 23000 loss: 0.2632331736832857\n",
      "  batch 24000 loss: 0.26564639154076575\n",
      "  batch 25000 loss: 0.2749020511806011\n",
      "  batch 26000 loss: 0.2794709716439247\n",
      "  batch 27000 loss: 0.2853064425289631\n",
      "  batch 28000 loss: 0.29027658776938914\n",
      "  batch 29000 loss: 0.2688494500666857\n",
      "  batch 30000 loss: 0.25334042590856554\n",
      "  batch 31000 loss: 0.24970011699199676\n",
      "LOSS train 0.24970011699199676 valid 0.23969045281410217\n",
      "EPOCH 4:\n",
      "  batch 1000 loss: 0.24621378745138645\n",
      "  batch 2000 loss: 0.24649100796878337\n",
      "  batch 3000 loss: 0.2497505750209093\n",
      "  batch 4000 loss: 0.24612710836529733\n",
      "  batch 5000 loss: 0.2488726374655962\n",
      "  batch 6000 loss: 0.2470375081151724\n",
      "  batch 7000 loss: 0.25221990250051024\n",
      "  batch 8000 loss: 0.2519889526516199\n",
      "  batch 9000 loss: 0.24734923554956914\n",
      "  batch 10000 loss: 0.25283222967386243\n",
      "  batch 11000 loss: 0.2465531707406044\n",
      "  batch 12000 loss: 0.24803627157211303\n",
      "  batch 13000 loss: 0.2519119869917631\n",
      "  batch 14000 loss: 0.2539529238641262\n",
      "  batch 15000 loss: 0.24696689835190774\n",
      "  batch 16000 loss: 0.2634096039682627\n",
      "  batch 17000 loss: 0.24716389894485474\n",
      "  batch 18000 loss: 0.2475994224101305\n",
      "  batch 19000 loss: 0.24940042424201966\n",
      "  batch 20000 loss: 0.2536455852985382\n",
      "  batch 21000 loss: 0.25068737933039664\n",
      "  batch 22000 loss: 0.2547860722094774\n",
      "  batch 23000 loss: 0.24559553998708725\n",
      "  batch 24000 loss: 0.2425419788211584\n",
      "  batch 25000 loss: 0.24615594770014287\n",
      "  batch 26000 loss: 0.24741472862660885\n",
      "  batch 27000 loss: 0.2432876064926386\n",
      "  batch 28000 loss: 0.25238753300905226\n",
      "  batch 29000 loss: 0.24246972240507603\n",
      "  batch 30000 loss: 0.24671581687033176\n",
      "  batch 31000 loss: 0.24983673480153085\n",
      "LOSS train 0.24983673480153085 valid 0.24105685949325562\n",
      "EPOCH 5:\n",
      "  batch 1000 loss: 0.24307898809015752\n",
      "  batch 2000 loss: 0.23990800414979457\n",
      "  batch 3000 loss: 0.27150951664149764\n",
      "  batch 4000 loss: 0.2728279846459627\n",
      "  batch 5000 loss: 0.25373555248975754\n",
      "  batch 6000 loss: 0.2709490294456482\n",
      "  batch 7000 loss: 0.25644920928776266\n",
      "  batch 8000 loss: 0.25088408941030504\n",
      "  batch 9000 loss: 0.253145397529006\n",
      "  batch 10000 loss: 0.2523591131716967\n",
      "  batch 11000 loss: 0.2527165952920914\n",
      "  batch 12000 loss: 0.24906244002282618\n",
      "  batch 13000 loss: 0.269792555347085\n",
      "  batch 14000 loss: 0.24324773924052714\n",
      "  batch 15000 loss: 0.24319226363301277\n",
      "  batch 16000 loss: 0.25204846657812596\n",
      "  batch 17000 loss: 0.24114124946296214\n",
      "  batch 18000 loss: 0.2525859743207693\n",
      "  batch 19000 loss: 0.24798271583020687\n",
      "  batch 20000 loss: 0.2546670327186584\n",
      "  batch 21000 loss: 0.25233750784397124\n",
      "  batch 22000 loss: 0.25908365681767465\n",
      "  batch 23000 loss: 0.2482671990096569\n",
      "  batch 24000 loss: 0.24768510104715824\n",
      "  batch 25000 loss: 0.2561529262363911\n",
      "  batch 26000 loss: 0.25055280050635337\n",
      "  batch 27000 loss: 0.2622400080859661\n",
      "  batch 28000 loss: 0.2548289538323879\n",
      "  batch 29000 loss: 0.33203709627687933\n",
      "  batch 30000 loss: 0.38346179157495497\n",
      "  batch 31000 loss: 0.34583712005615236\n",
      "LOSS train 0.34583712005615236 valid 0.31818675994873047\n",
      "EPOCH 6:\n",
      "  batch 1000 loss: 0.3651196609437466\n",
      "  batch 2000 loss: 0.527809532880783\n",
      "  batch 3000 loss: 0.5921402499675751\n",
      "  batch 4000 loss: 0.5929188109636306\n",
      "  batch 5000 loss: 0.5926908285617828\n",
      "  batch 6000 loss: 0.5926899780631065\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(epochs)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mEPOCH \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[33m'\u001b[39m.format(epoch + \u001b[32m1\u001b[39m))\n\u001b[32m      9\u001b[39m model.train(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m avg_loss = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m running_vloss = \u001b[32m0.0\u001b[39m\n\u001b[32m     15\u001b[39m model.eval()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(epoch_index)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m#    B, L, C = outputs.shape\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m#    output_logits = outputs.view(B*L, C).to(device)\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m#    target_flattened = shifted.view(B*L).to(device).long()\u001b[39;00m\n\u001b[32m     19\u001b[39m \n\u001b[32m     20\u001b[39m \n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m#    loss = loss_fn(output_logits, target_flattened)\u001b[39;00m\n\u001b[32m     23\u001b[39m     loss = calculate_loss_for_reliable_bits_only(ip2, op, outputs, loss_fn)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m     torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=\u001b[32m1.0\u001b[39m)\n\u001b[32m     28\u001b[39m     optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Code/mamba-seq-pred/venv/lib/python3.12/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Code/mamba-seq-pred/venv/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Code/mamba-seq-pred/venv/lib/python3.12/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train(epochs=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
