{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "589f41f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anjal/Desktop/Code/mamba-seq-pred/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3])\n",
      "tensor([[-0.7251, -0.7145, -0.4168]], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "from dataset import PolarDecDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from models.wrappers.mamba_32bits import MambaPolarDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d85178a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e6fb613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78f1e95",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36e1bd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PolarDecDataset(snr_db=6, num_samples=1000000)\n",
    "test_set = PolarDecDataset(snr_db=6, num_samples=1000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdad0b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset, batch_size = 32)\n",
    "test_dataloader = DataLoader(test_set, batch_size = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbbd230",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17a2a925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MambaPolarDecoder(\n",
       "  (discrete_embedding): Embedding(2, 16)\n",
       "  (linear_embedding1): Linear(in_features=1, out_features=16, bias=True)\n",
       "  (linear_embedding2): Linear(in_features=1, out_features=16, bias=True)\n",
       "  (encoder_layers): ModuleList(\n",
       "    (0): BiMambaEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x BiMambaBlock(\n",
       "          (pre_ln_f): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (mamba_f): Mamba(\n",
       "            (in_proj): Linear(in_features=16, out_features=64, bias=False)\n",
       "            (conv1d): Conv1d(32, 32, kernel_size=(4,), stride=(1,), padding=(3,), groups=32)\n",
       "            (act): SiLU()\n",
       "            (x_proj): Linear(in_features=32, out_features=33, bias=False)\n",
       "            (dt_proj): Linear(in_features=1, out_features=32, bias=True)\n",
       "            (out_proj): Linear(in_features=32, out_features=16, bias=False)\n",
       "          )\n",
       "          (post_ln_f): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (ffn_f): Sequential(\n",
       "            (0): Linear(in_features=16, out_features=64, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=64, out_features=16, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (pre_ln_r): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_ln_r): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (mamba_r): Mamba(\n",
       "            (in_proj): Linear(in_features=16, out_features=64, bias=False)\n",
       "            (conv1d): Conv1d(32, 32, kernel_size=(4,), stride=(1,), padding=(3,), groups=32)\n",
       "            (act): SiLU()\n",
       "            (x_proj): Linear(in_features=32, out_features=33, bias=False)\n",
       "            (dt_proj): Linear(in_features=1, out_features=32, bias=True)\n",
       "            (out_proj): Linear(in_features=32, out_features=16, bias=False)\n",
       "          )\n",
       "          (ffn_r): Sequential(\n",
       "            (0): Linear(in_features=16, out_features=64, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=64, out_features=16, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  (final_proj_layer): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MambaPolarDecoder(\n",
    "    d_model=16,\n",
    "    num_layer_encoder=1,\n",
    "    num_layers_bimamba_block=4,\n",
    "    seq_len=N,\n",
    "    d_state=16,\n",
    "    d_conv=4,\n",
    "    expand=2\n",
    ").to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f20a282",
   "metadata": {},
   "source": [
    "## Minor modification to the Loss Function: Calculates loss only at non frozen positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05e72cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss_for_reliable_bits_only(frozen_bit_prior, target_vector, predicted_vector, loss_fn):\n",
    "    \"\"\"\n",
    "    frozen_bit_prior: tensor of shape (seq_len,) with 1 for frozen, 0 for message bits\n",
    "    target_vector: tensor of shape (seq_len,)\n",
    "    predicted_vector: tensor of shape (seq_len,)\n",
    "    loss_fn: PyTorch loss function\n",
    "    \"\"\"\n",
    "    mask = (frozen_bit_prior != 1) \n",
    "    \n",
    "   \n",
    "    reliable_target = target_vector[mask]\n",
    "    reliable_predicted = predicted_vector[mask]\n",
    "\n",
    "   # print(f\"Length of reliable bits: {len(reliable_target)}\")\n",
    "\n",
    "    return loss_fn(reliable_predicted, reliable_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "230e81dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808bd9a5",
   "metadata": {},
   "source": [
    "## Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5dc9380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel Observation Vector: torch.Size([32, 32])\n",
      "Frozen Tensor: torch.Size([32, 32])\n",
      "\n",
      "Predicted Channel Input Vector(logits): torch.Size([32, 32])\n",
      "\n",
      "\n",
      "Predicted (sigmoid): tensor([[0.3038, 0.3824, 0.3246,  ..., 0.2908, 0.2673, 0.2454],\n",
      "        [0.4233, 0.3338, 0.3723,  ..., 0.2789, 0.3020, 0.2686],\n",
      "        [0.3510, 0.3955, 0.3669,  ..., 0.3993, 0.3035, 0.3530],\n",
      "        ...,\n",
      "        [0.3107, 0.3234, 0.3105,  ..., 0.2531, 0.2631, 0.2532],\n",
      "        [0.3796, 0.3411, 0.3047,  ..., 0.3040, 0.3243, 0.2731],\n",
      "        [0.3264, 0.3162, 0.4459,  ..., 0.3246, 0.3154, 0.3451]],\n",
      "       device='cuda:0', grad_fn=<SigmoidBackward0>)\n",
      "\n",
      "\n",
      "Predicted bits:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0][0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0][0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0][0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0][0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0][0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0][0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0][0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0][0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0][0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0][0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0][0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0][0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0][0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0][0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0][0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0][0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0][0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0][0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0][0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0][0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0][0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0][0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0][0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0][0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0][0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0][0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0][0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0][0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0][0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0][0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0][0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Loss: 0.7324300408363342\n"
     ]
    }
   ],
   "source": [
    "channel_tensor, frozen_tensor, snr_tensor, target_tensor= next(iter(train_dataloader))\n",
    "ip1 = channel_tensor.float().to(device)\n",
    "ip2 = frozen_tensor.int().to(device)\n",
    "ip3 = snr_tensor.float().to(device)\n",
    "\n",
    "predicted = model(ip1, ip2, ip3) #works\n",
    "\n",
    "loss = calculate_loss_for_reliable_bits_only(ip2, target_tensor.to(device), predicted.to(device), loss_fn) #works\n",
    "\n",
    "\n",
    "print(f\"Channel Observation Vector: {ip1.shape}\\nFrozen Tensor: {ip2.shape}\\n\")\n",
    "print(f\"Predicted Channel Input Vector(logits): {predicted.shape}\\n\\n\")\n",
    "\n",
    "print(f\"Predicted (sigmoid): {torch.sigmoid(predicted)}\\n\\n\")\n",
    "pred = (torch.sigmoid(predicted) > 0.5).long()\n",
    "\n",
    "print(f\"Predicted bits:{''.join(map(str, pred.cpu().tolist()))}\\n\")\n",
    "#print(f\"Actual bits: {''.join(str(int(i)) for i in target_tensor)}\\n\")\n",
    "print(f\"Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69534924",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32f8dbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index):\n",
    "\n",
    "    running_loss = 0\n",
    "    last_loss = 0\n",
    "\n",
    "\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        channel_tensor, frozen_tensor, snr_tensor, target_tensor = data\n",
    "        ip1 = channel_tensor.float().to(device)\n",
    "        ip2 = frozen_tensor.int().to(device)\n",
    "        ip3 = snr_tensor.float().to(device)\n",
    "        op = target_tensor.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(ip1,ip2 ,ip3 ).to(device)\n",
    "\n",
    "    #    B, L, C = outputs.shape\n",
    "    #    output_logits = outputs.view(B*L, C).to(device)\n",
    "    #    target_flattened = shifted.view(B*L).to(device).long()\n",
    "\n",
    "\n",
    "    #    loss = loss_fn(output_logits, target_flattened)\n",
    "        \n",
    "        loss = calculate_loss_for_reliable_bits_only(ip2, op, outputs, loss_fn)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i%1000 == 999:\n",
    "            last_loss = running_loss/1000\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            running_loss = 0.\n",
    "    return last_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f26fb8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train( epochs=50):\n",
    "\n",
    "    best_vloss = 1_000_000.\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "     print('EPOCH {}:'.format(epoch + 1))\n",
    " \n",
    "   \n",
    "     model.train(True)\n",
    "     avg_loss = train_one_epoch(epoch)\n",
    "\n",
    "\n",
    "     running_vloss = 0.0\n",
    "    \n",
    "     model.eval()\n",
    "\n",
    "   \n",
    "     with torch.no_grad():\n",
    "        for i, vdata in enumerate(test_dataloader):\n",
    "            vchannel_tensor, vfrozen_tensor, vsnr_tensor, vtarget_tensor = vdata\n",
    "            voutputs = model(vchannel_tensor.float().to(device), vfrozen_tensor.int().to(device), vsnr_tensor.float().to(device))\n",
    "          #  B, L, C = voutputs.shape\n",
    "          #  vloss = loss_fn(voutputs.view(B*L, C).to(device), vlabels.view(B*L).to(device))\n",
    "            \n",
    "            vloss = calculate_loss_for_reliable_bits_only(vfrozen_tensor.to(device), vtarget_tensor.to(device), voutputs.to(device), loss_fn)\n",
    "            running_vloss += vloss\n",
    "\n",
    "     avg_vloss = running_vloss / (i + 1)\n",
    "     print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "     if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = './checkpoints/model_epoch_{}'.format(epoch)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "     \n",
    "    print(\"Training completed. Model available to use\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d589f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 1000 loss: 0.3939847068488598\n",
      "  batch 2000 loss: 0.3761376224756241\n",
      "  batch 3000 loss: 0.3793273172080517\n",
      "  batch 4000 loss: 0.37432436341047287\n",
      "  batch 5000 loss: 0.3664797570109367\n",
      "  batch 6000 loss: 0.36705457988381385\n",
      "  batch 7000 loss: 0.3666394959688187\n",
      "  batch 8000 loss: 0.35937456831336023\n",
      "  batch 9000 loss: 0.3599668442606926\n",
      "  batch 10000 loss: 0.3585135807991028\n",
      "  batch 11000 loss: 0.3588571336567402\n",
      "  batch 12000 loss: 0.36129762449860575\n",
      "  batch 13000 loss: 0.35614758130908014\n",
      "  batch 14000 loss: 0.3397854159474373\n",
      "  batch 15000 loss: 0.3305469160676002\n",
      "  batch 16000 loss: 0.3245115993618965\n",
      "  batch 17000 loss: 0.3175469451248646\n",
      "  batch 18000 loss: 0.31661248591542246\n",
      "  batch 19000 loss: 0.31736321476101875\n",
      "  batch 20000 loss: 0.3180165456533432\n",
      "  batch 21000 loss: 0.31625718903541566\n",
      "  batch 22000 loss: 0.31554092955589297\n",
      "  batch 23000 loss: 0.31466167032718656\n",
      "  batch 24000 loss: 0.6543365717232227\n",
      "  batch 25000 loss: 0.6932442944645881\n",
      "  batch 26000 loss: 0.6932749692201614\n",
      "  batch 27000 loss: 0.6932521150708199\n",
      "  batch 28000 loss: 0.6933011921644211\n",
      "  batch 29000 loss: 0.6932212463617324\n",
      "  batch 30000 loss: 0.6932352567911149\n",
      "  batch 31000 loss: 0.6932193039059639\n",
      "LOSS train 0.6932193039059639 valid 0.6932724118232727\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory ./checkpoints does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(epochs)\u001b[39m\n\u001b[32m     36\u001b[39m     best_vloss = avg_vloss\n\u001b[32m     37\u001b[39m     model_path = \u001b[33m'\u001b[39m\u001b[33m./checkpoints/model_epoch_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m'\u001b[39m.format(epoch)\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining completed. Model available to use\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Code/mamba-seq-pred/venv/lib/python3.12/site-packages/torch/serialization.py:966\u001b[39m, in \u001b[36msave\u001b[39m\u001b[34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[39m\n\u001b[32m    963\u001b[39m     f = os.fspath(f)\n\u001b[32m    965\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[32m--> \u001b[39m\u001b[32m966\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[32m    967\u001b[39m         _save(\n\u001b[32m    968\u001b[39m             obj,\n\u001b[32m    969\u001b[39m             opened_zipfile,\n\u001b[32m   (...)\u001b[39m\u001b[32m    972\u001b[39m             _disable_byteorder_record,\n\u001b[32m    973\u001b[39m         )\n\u001b[32m    974\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Code/mamba-seq-pred/venv/lib/python3.12/site-packages/torch/serialization.py:828\u001b[39m, in \u001b[36m_open_zipfile_writer\u001b[39m\u001b[34m(name_or_buffer)\u001b[39m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    827\u001b[39m     container = _open_zipfile_writer_buffer\n\u001b[32m--> \u001b[39m\u001b[32m828\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Code/mamba-seq-pred/venv/lib/python3.12/site-packages/torch/serialization.py:792\u001b[39m, in \u001b[36m_open_zipfile_writer_file.__init__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    785\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m    786\u001b[39m         torch._C.PyTorchFileWriter(\n\u001b[32m    787\u001b[39m             \u001b[38;5;28mself\u001b[39m.file_stream, get_crc32_options(), _get_storage_alignment()\n\u001b[32m    788\u001b[39m         )\n\u001b[32m    789\u001b[39m     )\n\u001b[32m    790\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    791\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m--> \u001b[39m\u001b[32m792\u001b[39m         \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_crc32_options\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_get_storage_alignment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    795\u001b[39m     )\n",
      "\u001b[31mRuntimeError\u001b[39m: Parent directory ./checkpoints does not exist."
     ]
    }
   ],
   "source": [
    "train(epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781d873e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 1])\n",
      "Output: tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 0])\n",
      "\n",
      "Predicted (logits): tensor([[-12.3343,  12.1840, -12.3455, -12.3638, -12.3470, -12.3511, -12.3607,\n",
      "         -12.3202,  12.1719,  -0.0603]], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Predicted (sigmoid): tensor([[4.3982e-06, 9.9999e-01, 4.3493e-06, 4.2704e-06, 4.3428e-06, 4.3248e-06,\n",
      "         4.2838e-06, 4.4606e-06, 9.9999e-01, 4.8492e-01]], device='cuda:0',\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "Predicted bits:0100000010\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
